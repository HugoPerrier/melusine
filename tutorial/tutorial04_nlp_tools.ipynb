{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from tempfile import TemporaryDirectory, TemporaryFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Tools tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **nlp_tools** subpackage offers classic NLP tools implemented as classes that will be used to preprocess an already cleaned text :\n",
    "- a **Tokenizer class** : to split a sentence-like string into a list of sub-strings (tokens).\n",
    "- a **Phraser class** : to transform common multi-word expressions into single elements (*new york* becomes *new_york*)\n",
    "- an **Embedding class** : to represent of words in a lower dimensional vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melusine.data.data_loader import load_email_data\n",
    "df_emails = load_email_data(type=\"preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tokenizer class\n",
    "The Tokenizer class splits a sentence-like string into a list of sub-strings (tokens).  \n",
    "\n",
    "The arguments of a Tokenizer object are just the input_columns and output_columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melusine.nlp_tools.tokenizer import Tokenizer\n",
    "tokenizer = Tokenizer (input_column='clean_body', output_column=\"tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the **fit_transform** method on a dataframe to create a new ***tokens* column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base text\n",
      "je vous informe que la nouvelle immatriculation est enfin faite. je vous prie de trouver donc la carte grise ainsi que la nouvelle immatriculation. je vous demanderai de faire les changements necessaires concernant lassurance.\n",
      "\n",
      "Tokenized text\n",
      "['informe', 'nouvelle', 'immatriculation', 'enfin', 'faite', 'prie', 'trouver', 'donc', 'carte', 'grise', 'ainsi', 'nouvelle', 'immatriculation', 'demanderai', 'faire', 'les', 'changements', 'necessaires', 'concernant', 'lassurance']\n"
     ]
    }
   ],
   "source": [
    "df_emails = tokenizer.fit_transform(df_emails)\n",
    "print(\"Base text\")\n",
    "print(df_emails.clean_body[1])\n",
    "print(\"\\nTokenized text\")\n",
    "print(df_emails.tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load / Save a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TemporaryDirectory() as tmpdir:\n",
    "    path = f\"{tmpdir}/tokenizer.pkl\"\n",
    "    _ = joblib.dump(tokenizer, path, compress=True)\n",
    "    tokenizer_reload = joblib.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emails = tokenizer_reload.fit_transform(df_emails)\n",
    "print(df_emails.tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Phraser class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Phraser class transforms common multi-word expressions into single elements: for example *new york* becomes *new_york*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments of a Phraser object are:\n",
    "- **input_column :** the name of the column of the dataframe that will be used as input for the training of the Phraser.\n",
    "- **common_terms :** list of stopwords to be ignored. The default list is defined in the *conf.json* file.\n",
    "- **threshold :** threshold to select collocations.\n",
    "- **min_count :** minimum count of word to be selected as collocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.104064Z",
     "start_time": "2019-09-03T13:16:54.128719Z"
    }
   },
   "outputs": [],
   "source": [
    "from melusine.nlp_tools.phraser import Phraser\n",
    "\n",
    "phraser = Phraser(\n",
    "    input_column='tokens',\n",
    "    output_column='phrased_tokens',\n",
    "    threshold=5,\n",
    "    min_count=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a phraser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input dataframe must contain a column with a clean text : **a sentence-like string with only lowcase letters and no accents**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.190813Z",
     "start_time": "2019-09-03T13:16:56.150379Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = phraser.fit(df_emails)\n",
    "df_emails = phraser.transform(df_emails)\n",
    "print(df_emails['phrased_tokens'].iloc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected result : You should see phrased tokens such as  \n",
    "\"bulletin\" + \"salaire\" = \"bulletin_salaire\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load/Save a phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.205955Z",
     "start_time": "2019-09-03T13:16:56.192989Z"
    }
   },
   "outputs": [],
   "source": [
    "with TemporaryDirectory() as tmpdir:\n",
    "    path = f\"{tmpdir}/phraser.pkl\"\n",
    "    _ = joblib.dump(phraser, path, compress=True)\n",
    "    phraser_reload = joblib.load(path)\n",
    "    \n",
    "print(phraser_reload.transform(df_emails)['phrased_tokens'].iloc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Embedding class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are abstract representations of words in a lower dimensional vector space. One of the advantages of word embeddings is thus to save computational cost. The Melusine Embedding class uses a **Word2Vec** model. The trained Embedding object will be used in the Models subpackage to train a Neural Network to classify emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments of an Embedding object are :\n",
    "- **input_column :** the name of the column used as an input for the training.\n",
    "- **workers :** the number of cores used for computation. Default value, 40.\n",
    "- **seed :** seed for the embedding model,\n",
    "- **iter :** number of iterations for the training,\n",
    "- **size :** dimension of the embeddings\n",
    "- **window :** \n",
    "- **min_count :** minimum number of occurences for a word to be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.940504Z",
     "start_time": "2019-09-03T13:16:56.707282Z"
    }
   },
   "outputs": [],
   "source": [
    "from melusine.nlp_tools.embedding import Embedding\n",
    "\n",
    "embedding = Embedding(\n",
    "    tokens_column='tokens',\n",
    "    size=300,\n",
    "    workers=4,\n",
    "    min_count=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.983525Z",
     "start_time": "2019-09-03T13:16:56.942514Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding.train(df_emails)\n",
    "embedding.embedding.most_similar(\"vehicule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning : The embedding is trained on a very small dataset so the results here are irrelevant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpe_melusine",
   "language": "python",
   "name": "hpe_melusine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from tempfile import TemporaryDirectory, TemporaryFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Tools tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **nlp_tools** subpackage offers classic NLP tools implemented as classes that will be used to preprocess an already cleaned text :\n",
    "- a **Tokenizer class** : to split a sentence-like string into a list of sub-strings (tokens).\n",
    "- a **Phraser class** : to transform common multi-word expressions into single elements (*new york* becomes *new_york*)\n",
    "- an **Embedding class** : to represent of words in a lower dimensional vector space.\n",
    "- a **Stemmer class**: to reduce words to their word stem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melusine.data.data_loader import load_email_data\n",
    "df_emails = load_email_data(type=\"preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tokenizer class\n",
    "The Tokenizer class splits a sentence-like string into a list of sub-strings (tokens).  \n",
    "\n",
    "The arguments of a Tokenizer object are just the input_columns and output_columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melusine.nlp_tools.tokenizer import Tokenizer\n",
    "tokenizer = Tokenizer (input_column='clean_body', output_column=\"tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the **fit_transform** method on a dataframe to create a new ***tokens* column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base text\n",
      "je vous informe que la nouvelle immatriculation est enfin faite. je vous prie de trouver donc la carte grise ainsi que la nouvelle immatriculation. je vous demanderai de faire les changements necessaires concernant lassurance.\n",
      "\n",
      "Tokenized text\n",
      "['informe', 'nouvelle', 'immatriculation', 'enfin', 'faite', 'prie', 'trouver', 'donc', 'carte', 'grise', 'ainsi', 'nouvelle', 'immatriculation', 'demanderai', 'faire', 'les', 'changements', 'necessaires', 'concernant', 'lassurance']\n"
     ]
    }
   ],
   "source": [
    "df_emails = tokenizer.fit_transform(df_emails)\n",
    "print(\"Base text\")\n",
    "print(df_emails.clean_body[1])\n",
    "print(\"\\nTokenized text\")\n",
    "print(df_emails.tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load / Save a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TemporaryDirectory() as tmpdir:\n",
    "    path = f\"{tmpdir}/tokenizer.pkl\"\n",
    "    _ = joblib.dump(tokenizer, path, compress=True)\n",
    "    tokenizer_reload = joblib.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emails = tokenizer_reload.fit_transform(df_emails)\n",
    "print(df_emails.tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Phraser class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Phraser class transforms common multi-word expressions into single elements: for example *new york* becomes *new_york*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments of a Phraser object are:\n",
    "- **input_column :** the name of the column of the dataframe that will be used as input for the training of the Phraser.\n",
    "- **common_terms :** list of stopwords to be ignored. The default list is defined in the *conf.json* file.\n",
    "- **threshold :** threshold to select collocations.\n",
    "- **min_count :** minimum count of word to be selected as collocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.104064Z",
     "start_time": "2019-09-03T13:16:54.128719Z"
    }
   },
   "outputs": [],
   "source": [
    "from melusine.nlp_tools.phraser import Phraser\n",
    "\n",
    "phraser = Phraser(\n",
    "    input_column='tokens',\n",
    "    output_column='phrased_tokens',\n",
    "    threshold=5,\n",
    "    min_count=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a phraser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input dataframe must contain a column with a clean text : **a sentence-like string with only lowcase letters and no accents**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.190813Z",
     "start_time": "2019-09-03T13:16:56.150379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fais', 'suite_a', 'mail', 'envoye', 'bulletin_salaire', 'courrier', 'semblerait', 'receptionne', 'trouverez', 'ci-joint', 'bulletin_salaire']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/78169t/.conda/envs/melusine/lib/python3.6/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "_ = phraser.fit(df_emails)\n",
    "df_emails = phraser.transform(df_emails)\n",
    "print(df_emails['phrased_tokens'].iloc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected result : You should see phrased tokens such as  \n",
    "\"bulletin\" + \"salaire\" = \"bulletin_salaire\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load/Save a phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.205955Z",
     "start_time": "2019-09-03T13:16:56.192989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fais', 'suite_a', 'mail', 'envoye', 'bulletin_salaire', 'courrier', 'semblerait', 'receptionne', 'trouverez', 'ci-joint', 'bulletin_salaire']\n"
     ]
    }
   ],
   "source": [
    "with TemporaryDirectory() as tmpdir:\n",
    "    path = f\"{tmpdir}/phraser.pkl\"\n",
    "    _ = joblib.dump(phraser, path, compress=True)\n",
    "    phraser_reload = joblib.load(path)\n",
    "    \n",
    "print(phraser_reload.transform(df_emails)['phrased_tokens'].iloc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Embedding class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are abstract representations of words in a lower dimensional vector space. One of the advantages of word embeddings is thus to save computational cost. The Melusine Embedding class uses a **Word2Vec** model. The trained Embedding object will be used in the Models subpackage to train a Neural Network to classify emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments of an Embedding object are :\n",
    "- **input_column :** the name of the column used as an input for the training.\n",
    "- **workers :** the number of cores used for computation. Default value, 40.\n",
    "- **seed :** seed for the embedding model,\n",
    "- **iter :** number of iterations for the training,\n",
    "- **size :** dimension of the embeddings\n",
    "- **window :** \n",
    "- **min_count :** minimum number of occurences for a word to be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.940504Z",
     "start_time": "2019-09-03T13:16:56.707282Z"
    }
   },
   "outputs": [],
   "source": [
    "from melusine.nlp_tools.embedding import Embedding\n",
    "\n",
    "embedding = Embedding(\n",
    "    tokens_column='tokens',\n",
    "    size=300,\n",
    "    workers=4,\n",
    "    min_count=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.983525Z",
     "start_time": "2019-09-03T13:16:56.942514Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('contrat', 0.16994555294513702),\n",
       " ('imaginaire', 0.13840633630752563),\n",
       " ('trouver', 0.11760054528713226),\n",
       " ('bulletin', 0.11261097341775894),\n",
       " ('2', 0.09284727275371552),\n",
       " ('salaire', 0.09037808328866959),\n",
       " ('entretien', 0.060972344130277634),\n",
       " ('flag_time_', 0.05960045009851456),\n",
       " ('concernant', 0.059587351977825165),\n",
       " ('voici', 0.05908174812793732)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.train(df_emails)\n",
    "embedding.embedding.most_similar(\"vehicule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning : The embedding is trained on a very small dataset so the results here are irrelevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The stemmer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stemmer class reduces words (tokens) from a list to their word stem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments of a Stemmer object are:\n",
    "- **input_column :** the name of the column of the dataframe that will be used as input for the reduction of the Stemmer.\n",
    "- **output_column :** the name of the column of the dataframe that will be used as output for the reduction of the Stemmer.\n",
    "- **language :** language used to stem words, default \"french\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melusine.nlp_tools.stemmer import Stemmer\n",
    "\n",
    "stemmer = Stemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stemmer doesn't need any training. You can apply it directly on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fais', 'suit', 'a', 'mail', 'envoy', 'bulletin', 'salair', 'courri', 'sembl', 'reception', 'trouv', 'ci-joint', 'bulletin', 'salair']\n"
     ]
    }
   ],
   "source": [
    "df_emails = stemmer.transform(df_emails)\n",
    "print(df_emails['stemmed_tokens'].iloc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The lemmatizer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lemmatizer class changes words from a string by the base or dictionary form of a word, which is known as the lemma.\n",
    "\n",
    "The arguments of a Lemmatizer object are:\n",
    "\n",
    "- **in_col :** the name of the column of the dataframe that will be used as input for lemmatization.\n",
    "- **out_col :** the name of the column of the dataframe that will be used as output for lemmatization.\n",
    "- **engine :** lemmatization engine to load. Choices are 'spacy' and 'Lefff'.\n",
    "- **engine_conf:** Spacy model to load under the hood. For french, choices are 'fr_core_news_sm', 'fr_core_news_md',     'fr_core_news_lg'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get the following error:\n",
    "\n",
    "<span style=\"color:red\">MaxRetryError: HTTPSConnectionPool(host='github.com', port=443): Max retries exceeded with url: \n",
    "/sammous/spacy-lefff-model/releases/latest/download/model.tar.gz (Caused by \n",
    "NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc058df3fd0>: Failed to establish a new connection: \n",
    "[Errno 110] Connection timed out'))</span>\n",
    "\n",
    "This may due to the use of proxy.                   \n",
    "Please download locally the model: https://github.com/sammous/spacy-lefff-model/releases/latest/download/model.tar.gz \n",
    "\n",
    "To do so: \n",
    "Go to the repository: <site-packages>/spacy_lefff/data/tagger\n",
    "Download and unzip the model with\n",
    "    \n",
    "    >>wget https://github.com/sammous/spacy-lefff-model/releases/latest/download/model.tar.gz\n",
    "    \n",
    "    >>tar xvfz model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melusine.nlp_tools.lemmatizer import Lemmatizer\n",
    "spacy_lemma_sm = Lemmatizer('clean_body', 'lemma_spacy_sm', engine='spacy', engine_conf='fr_core_news_sm')\n",
    "lefff_lemma = Lemmatizer('clean_body', 'lemma_lefff', engine='Lefff', engine_conf='fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je faire suite avoir votre mail . je avoir envoye mon bulletin de salaire par courrier mais il sembler que vous ne l' ayez pas receptionne . vous trouver ci - joindre mon bulletin de salaire .\n"
     ]
    }
   ],
   "source": [
    "df_emails = spacy_lemma_sm.fit_transform(df_emails)\n",
    "print(df_emails['lemma_spacy_sm'].iloc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je faire suite avoir son mail . j' avoir envoye son bulletin de salaire par courrier mais il sembler que vous ne l' ayez pas receptionne . vous trouver ci - joindre son bulletin de salaire .\n"
     ]
    }
   ],
   "source": [
    "df_emails = lefff_lemma.fit_transform(df_emails)\n",
    "print(df_emails['lemma_lefff'].iloc[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_melusine37",
   "language": "python",
   "name": "env_melusine37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
